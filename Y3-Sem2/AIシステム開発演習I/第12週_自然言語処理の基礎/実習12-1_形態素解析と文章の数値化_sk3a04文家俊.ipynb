{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["実習12-1 形態素解析と文の数値化\n","---\n","自然言語処理を行う上で、ニューラルネットワークやその他アルゴリズムで計算を行うためには、言語を数値化する必要がある。\n","\n","今回は、自然言語処理の最初のステップである分かち書き、形態素解析、数値化を中心に、前処理の一部を確認する。\n"],"metadata":{"id":"pkLtBMTMGd21"}},{"cell_type":"markdown","source":["# 1. 形態素解析\n","\n","形態素解析とは、自然言語処理（NLP）の一種で、文章を意味を持つ最小単位である形態素に分割し、品詞を判別する処理である。\n","\n","自然言語処理では、最初のステップとして文章を形態素に分割する必要がある。また、用途に応じて、特定の品詞のみを抽出することもできる。"],"metadata":{"id":"1Yf3vIC5LF43"}},{"cell_type":"markdown","source":["## 1-1. 英語の形態素解析\n","\n","まずは、英語の形態素解析を行ってみる。英語の場合は、単語間がスペースで区切られており、比較的簡単に行うことが出来る。今回はPythonのライブラリであるNLTK（Natural Language Tool Kit）を使用する。"],"metadata":{"id":"vAyJK9WAML_Q"}},{"cell_type":"markdown","source":["### 1-1-1. ライブラリの使用準備（NLTK）\n","Google Colabにはnltkはインストール済のため、importすることはできるが、必要なリソースはnltk.download('リソース名')でダウンロードする必要がある。\n","\n","```\n","import nltk\n","nltk.download('punkt_tab') # 分かち書きに必要\n","nltk.download('averaged_perceptron_tagger_eng') # 品詞の取得に必要\n","```"],"metadata":{"id":"FOT2g5s0Mf8X"}},{"cell_type":"code","source":["# NLTKの準備\n","# 未インストールならpip install nltkなどでインストール\n","import nltk\n","nltk.download('punkt_tab') # 分かち書きに必要\n","nltk.download('averaged_perceptron_tagger_eng') # 品詞の取得に必要"],"metadata":{"id":"SvG6v4qYN1IK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736383290024,"user_tz":-540,"elapsed":6433,"user":{"displayName":"ブンカシュン文家俊","userId":"02523924470019961246"}},"outputId":"6c382efd-5ce8-4b88-d75b-3cd41f8776e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":["### 1-1-2. 分かち書きをする（NLTK）\n","単語ごとに分割してリストにする。\n","色々な文を分かち書きしてみよう。\n","```\n","nltk.word_tokenize(\"This is a pen.\")  # 直接入れてもいい\n","string = \"I am 20 years old.\"\n","nltk.word_tokenize(str) # 文字列変数を入れてもいい\n","# 後のことを考えて変数に入れておこう\n","```"],"metadata":{"id":"RJ8THGOUOB3m"}},{"cell_type":"code","source":["# 分かち書きをして、表示して確認する。\n","string = \"The early bird catches the worm.\"\n","words =  nltk.word_tokenize(string)\n","words\n","#-> ['The', 'early', 'bird', 'catches', 'the', 'worm', '.']"],"metadata":{"id":"JZlEEITlOHj0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736383375092,"user_tz":-540,"elapsed":596,"user":{"displayName":"ブンカシュン文家俊","userId":"02523924470019961246"}},"outputId":"a98fb86f-8c3b-4fc7-8b93-4df35da363de"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['The', 'early', 'bird', 'catches', 'the', 'worm', '.']"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["### 1-1-3. 品詞の取得（NLTK）\n","上で分かち書きしたものを、nltk.pos_tag()の引数に入れると、\n","単語と品詞をタプルにしたもののリストを返す。\n","```\n","nltk.pos_tag(分かち書きしたもの)\n","```"],"metadata":{"id":"KCNyoQ-VPKNm"}},{"cell_type":"code","source":["# 品詞の取得\n","string = \"The early bird catches the worm.\"\n","words =  nltk.word_tokenize(string)\n","nltk.pos_tag(words)\n","\n","[('The', 'DT'),\n"," ('early', 'JJ'),\n"," ('bird', 'NN'),\n"," ('catches', 'VBZ'),\n"," ('the', 'DT'),\n"," ('worm', 'NN'),\n"," ('.', '.')]\n"],"metadata":{"id":"PeGqQIG5PROm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736383407641,"user_tz":-540,"elapsed":370,"user":{"displayName":"ブンカシュン文家俊","userId":"02523924470019961246"}},"outputId":"83560a6f-91ef-455f-a9b5-3b302d8f4af1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('The', 'DT'),\n"," ('early', 'JJ'),\n"," ('bird', 'NN'),\n"," ('catches', 'VBZ'),\n"," ('the', 'DT'),\n"," ('worm', 'NN'),\n"," ('.', '.')]"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["※ 出力は、単語と品詞のタプルのリストになっています。特定の品詞の単語のみ取り出す場合の参考にしてください。"],"metadata":{"id":"eB4aH4DhRcov"}},{"cell_type":"markdown","source":["**[課題] 品詞の略称（NNなど）について、分かる範囲でいいので調べてください。**\n","\n","*   例）NNS：名詞（複数形）\n","*   \n","*   "],"metadata":{"id":"PLDiSEZ1Qqkf"}},{"cell_type":"markdown","source":["## 1-2. 日本語の形態素解析（MeCab）\n","次に、日本語の形態素解析を行ってみる。日本語は単語間がスペースで区切られておらず、自力で分かち書きを行うことが難しい。\n","\n","日本語の形態素解析ツールとしては、MeCab、juman、Janomeなどが有名である。\n","まずはMeCabを使用してみる。"],"metadata":{"id":"_ekiTHlNMgaB"}},{"cell_type":"markdown","source":["### 1-2-1. ライブラリの使用準備（Mecab）\n","Google ColabにはMeCabがインストールされていないようなので、まずはインストールする。\n","MeCabの本体（mecab-python3）と辞書（unidic-lite）をpipでインストールする。\n","```\n","# 本体と辞書をインストール\n","!pip install mecab-python3 unidic-lite\n","\n","```"],"metadata":{"id":"DdmE3-ybMhbi"}},{"cell_type":"markdown","source":["インストール後、importすると使用できる。\n","```\n","# 後のコードで一緒に実行する\n","import MeCab\n","```"],"metadata":{"id":"H-AHhzxd_TnY"}},{"cell_type":"code","source":["# 本体と辞書をインストール\n","pip install mecab-python3 unidic-lite"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jr1lTc0dh-3D","executionInfo":{"status":"ok","timestamp":1736383684474,"user_tz":-540,"elapsed":26111,"user":{"displayName":"ブンカシュン文家俊","userId":"02523924470019961246"}},"outputId":"554ec084-5e0d-44c8-b429-a3fe0cb8917b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mecab-python3\n","  Downloading mecab_python3-1.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n","Collecting unidic-lite\n","  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Downloading mecab_python3-1.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.7/581.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: unidic-lite\n","  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658818 sha256=106e63749b6102a9feb6094e68ba36e7dc102fb06dc1b336ba0c18fca4fc9128\n","  Stored in directory: /root/.cache/pip/wheels/89/e8/68/f9ac36b8cc6c8b3c96888cd57434abed96595d444f42243853\n","Successfully built unidic-lite\n","Installing collected packages: unidic-lite, mecab-python3\n","Successfully installed mecab-python3-1.0.10 unidic-lite-1.0.8\n"]}]},{"cell_type":"markdown","source":["### 1-2-2. 分かち書きをする（MeCab）\n","分かち書きをするために、MeCab.Taggerインスタンスを作成する。作成したインスタンスのparse()メソッドに分かち書きしたい文を入れる。\n","分かち書きのみを実施する場合は、インスタンス作成時の引数に\"-Owakati\"を指定する。\n","\n","```\n","import MeCab\n","text = \"すもももももももものうち\"\n","m = MeCab.Tagger(\"-Owakati\")\n","print(m.parse(text))\n","\n","```"],"metadata":{"id":"rcrUOIs-8Ays"}},{"cell_type":"code","source":["# MeCabを使用した分かち書き\n","import MeCab\n","text = \"すもももももももものうち\"\n","m = MeCab.Tagger(\"-Owakati\")\n","print(m.parse(text))"],"metadata":{"id":"eEGJjynfR1Jw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736383736696,"user_tz":-540,"elapsed":406,"user":{"displayName":"ブンカシュン文家俊","userId":"02523924470019961246"}},"outputId":"6f2c4d8b-8a43-4e63-d86c-9ada774a023f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["すもも も もも も もも の うち \n","\n"]}]},{"cell_type":"markdown","source":["※ 結果がひとつの文字列として出力されています。"],"metadata":{"id":"2geBqHXtR80X"}},{"cell_type":"markdown","source":["### 1-2-3. 品詞情報の取得（MeCab）\n","MeCab.Taggerは標準で品詞情報なども出力することができる。引数を指定せずにインスタンスを作成して、parseを実行してみる。\n","```\n","# インスタンス作成（引数指定なし）\n","m = MeCab.Tagger()\n","# 形態素解析の例\n","text = \"日本語の形態素解析は難しかったが少し分かった。\"\n","print(m.parse(text))\n","```"],"metadata":{"id":"KMX40_5-CpO_"}},{"cell_type":"code","source":["# インスタンス作成（引数指定なし）\n","m = MeCab.Tagger()\n","# 形態素解析の例\n","text = \"日本語の形態素解析は難しかったが少し分かった。\"\n","print(m.parse(text))"],"metadata":{"id":"ifg6T0XzDiGb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736384101861,"user_tz":-540,"elapsed":417,"user":{"displayName":"ブンカシュン文家俊","userId":"02523924470019961246"}},"outputId":"a6a5d842-a715-479b-a808-e7143c12efb5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["日本\tニッポン\tニッポン\t日本\t名詞-固有名詞-地名-国\t\t\t3\n","語\tゴ\tゴ\t語\t名詞-普通名詞-一般\t\t\t1\n","の\tノ\tノ\tの\t助詞-格助詞\t\t\t\n","形態\tケータイ\tケイタイ\t形態\t名詞-普通名詞-一般\t\t\t0\n","素\tソ\tソ\t素\t接尾辞-名詞的-一般\t\t\t\n","解析\tカイセキ\tカイセキ\t解析\t名詞-普通名詞-サ変可能\t\t\t0\n","は\tワ\tハ\tは\t助詞-係助詞\t\t\t\n","難しかっ\tムズカシカッ\tムズカシイ\t難しい\t形容詞-一般\t形容詞\t連用形-促音便\t0,4\n","た\tタ\tタ\tた\t助動詞\t助動詞-タ\t終止形-一般\t\n","が\tガ\tガ\tが\t助詞-接続助詞\t\t\t\n","少し\tスコシ\tスコシ\t少し\t副詞\t\t\t2\n","分かっ\tワカッ\tワカル\t分かる\t動詞-一般\t五段-ラ行\t連用形-促音便\t2\n","た\tタ\tタ\tた\t助動詞\t助動詞-タ\t終止形-一般\t\n","。\t\t\t。\t補助記号-句点\t\t\t\n","EOS\n","\n"]}]},{"cell_type":"markdown","source":["※ 結果がひとつの文字列として出力されています。"],"metadata":{"id":"AnJGhJgbSGEn"}},{"cell_type":"markdown","source":["## 1-3. 日本語の形態素解析（Janome）\n","次に、Janomeを使用して日本語の分かち書きと形態素解析を実施してみる。MeCabの場合とはほぼ同じだが、出力形式などが異なるため注意する。"],"metadata":{"id":"2XTTQJQEFm26"}},{"cell_type":"markdown","source":["### 1-3-1. ライブラリの使用準備（Janome）\n","Google ColabにはJanomeがインストールされていないようなので、まずはpipでインストールする。\n","```\n","# Janomeをインストール\n","!pip install janome\n","\n","```"],"metadata":{"id":"nVmupFGaGVMM"}},{"cell_type":"code","source":["# Janomeをインストール\n","!pip install janome"],"metadata":{"id":"qhcQNPk3Hw3T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736384392386,"user_tz":-540,"elapsed":20880,"user":{"displayName":"ブンカシュン文家俊","userId":"02523924470019961246"}},"outputId":"a5cea8d8-e02c-432e-d89d-25772830ba66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting janome\n","  Downloading Janome-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\n","Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: janome\n","Successfully installed janome-0.5.0\n"]}]},{"cell_type":"markdown","source":["### 1-3-2. 分かち書きをする（Janome）\n","\n","分かち書きをするために、 janome.tokenizer.Tokenizer()インスタンスを作成する。作成したインスタンスのtokenize()メソッドに分かち書きしたい文を入れる。分かち書きのみを実施したい場合は、インスタンス作成時またはtokenize時のオプションで「wakati=True」を指定する。\n","\n","ただし、**分かち書きした結果は文字列ではなくオブジェクトであるため、for文を使って出力する。**\n","\n","```\n","# 書き方は様々あり\n","from janome.tokenizer import Tokenizer\n","text = \"すもももももももものうち\"\n","j = Tokenizer(wakati=True)\n","for token in j.tokenize(text):\n","  print(token)\n","```"],"metadata":{"id":"Gs9WhXVDIRXY"}},{"cell_type":"code","source":["# 書き方は様々あり\n","from janome.tokenizer import Tokenizer\n","text = \"すもももももももものうち\"\n","j = Tokenizer(wakati=True)\n","for token in j.tokenize(text):\n","  print(token)"],"metadata":{"id":"rQ2CSMFOQlO6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736384445243,"user_tz":-540,"elapsed":373,"user":{"displayName":"ブンカシュン文家俊","userId":"02523924470019961246"}},"outputId":"3d428feb-576a-43c3-912f-9cbeda469fdd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["すもも\n","も\n","もも\n","も\n","もも\n","の\n","うち\n"]}]},{"cell_type":"markdown","source":["※ 各tokenは文字列で出力されています。"],"metadata":{"id":"zzIJTQtiSTtk"}},{"cell_type":"markdown","source":["### 1-3-3. 品詞情報の取得（Janome）\n"," janome.tokenizer.Tokenizer()は標準で品詞情報なども出力することができる。引数を指定せずにインスタンスを作成して、tokenizeを実行してみる。\n"],"metadata":{"id":"6u0Y7gqBOzde"}},{"cell_type":"code","source":["# 書き方は様々あり\n","from janome.tokenizer import Tokenizer\n","text = \"すもももももももものうち\"\n","j = Tokenizer()\n","for token in j.tokenize(text):\n","  print(token)"],"metadata":{"id":"qdZvmeEdJz0j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736384504911,"user_tz":-540,"elapsed":437,"user":{"displayName":"ブンカシュン文家俊","userId":"02523924470019961246"}},"outputId":"3e35d3cd-88b6-4404-ed86-f61bf938752c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["すもも\t名詞,一般,*,*,*,*,すもも,スモモ,スモモ\n","も\t助詞,係助詞,*,*,*,*,も,モ,モ\n","もも\t名詞,一般,*,*,*,*,もも,モモ,モモ\n","も\t助詞,係助詞,*,*,*,*,も,モ,モ\n","もも\t名詞,一般,*,*,*,*,もも,モモ,モモ\n","の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n","うち\t名詞,非自立,副詞可能,*,*,*,うち,ウチ,ウチ\n"]}]},{"cell_type":"markdown","source":["※ 出力は、janome.tokenizer.Tokenクラスであり、各要素には、以下のようにアクセスできる。\n","\n","- `surface`: 表層形（元の文章に出現する単語そのもの）\n","- `part_of_speech`: 品詞情報（名詞、動詞、形容詞など）\n","- `infl_type`: 活用型（動詞や形容詞の場合）\n","- `infl_form`: 活用形（動詞や形容詞の場合）\n","- `base_form`: 基本形（辞書形）\n","- `reading`: 読み（カタカナ）\n","- `phonetic`: 発音（カタカナ）\n","\n","```\n","# 各トークンの詳細情報（一部）\n","result = j.tokenize(\"日本語の形態素解析は難しかったが少し分かった。\")\n","for token in result:\n","    print(f\"表層形: {token.surface}\")\n","    print(f\"品詞: {token.part_of_speech}\")\n","    print(f\"基本形: {token.base_form}\")\n","    print(f\"読み: {token.reading}\")\n","    print(\"---\")\n","```"],"metadata":{"id":"grqUjirVh_F1"}},{"cell_type":"code","source":["# 各トークンの詳細情報（一部）\n","result = j.tokenize(\"日本語の形態素解析は難しかったが少し分かった。\")\n","for token in result:\n","    print(f\"表層形: {token.surface}\")\n","    print(f\"品詞: {token.part_of_speech}\")\n","    print(f\"基本形: {token.base_form}\")\n","    print(f\"読み: {token.reading}\")\n","    print(\"---\")"],"metadata":{"id":"G56MVuLpjkyQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736384510453,"user_tz":-540,"elapsed":524,"user":{"displayName":"ブンカシュン文家俊","userId":"02523924470019961246"}},"outputId":"dcc165d3-cf5d-4e0f-941f-6220b8d2574a","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["表層形: 日本語\n","品詞: 名詞,一般,*,*\n","基本形: 日本語\n","読み: ニホンゴ\n","---\n","表層形: の\n","品詞: 助詞,連体化,*,*\n","基本形: の\n","読み: ノ\n","---\n","表層形: 形態素\n","品詞: 名詞,一般,*,*\n","基本形: 形態素\n","読み: ケイタイソ\n","---\n","表層形: 解析\n","品詞: 名詞,サ変接続,*,*\n","基本形: 解析\n","読み: カイセキ\n","---\n","表層形: は\n","品詞: 助詞,係助詞,*,*\n","基本形: は\n","読み: ハ\n","---\n","表層形: 難しかっ\n","品詞: 形容詞,自立,*,*\n","基本形: 難しい\n","読み: ムズカシカッ\n","---\n","表層形: た\n","品詞: 助動詞,*,*,*\n","基本形: た\n","読み: タ\n","---\n","表層形: が\n","品詞: 助詞,接続助詞,*,*\n","基本形: が\n","読み: ガ\n","---\n","表層形: 少し\n","品詞: 副詞,助詞類接続,*,*\n","基本形: 少し\n","読み: スコシ\n","---\n","表層形: 分かっ\n","品詞: 動詞,自立,*,*\n","基本形: 分かる\n","読み: ワカッ\n","---\n","表層形: た\n","品詞: 助動詞,*,*,*\n","基本形: た\n","読み: タ\n","---\n","表層形: 。\n","品詞: 記号,句点,*,*\n","基本形: 。\n","読み: 。\n","---\n"]}]},{"cell_type":"markdown","source":["# 2. 文章の数値化\n","文章を機械学習やディープラーニングに入力する場合は、基本的に以下の処理を実施する。\n","- 分かち書き<br/>\n","  **※ 今回はすべて品詞の単語を使用するものとする。**\n","- 各要素を数値に変換\n","- 長さを一定にする（パディングまたはカット）\n","\n","今回は、Pythonのライブラリである[tensorflow.keras.preprocessing.text.Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)を使用して、ここまでの処理を実施する。"],"metadata":{"id":"-YEEvfY6nqr7"}},{"cell_type":"markdown","source":["## 2-1. 英文の数値化"],"metadata":{"id":"OhHdK8XSr41K"}},{"cell_type":"markdown","source":["### 2-1-1. 分かち書き（英語）\n","\n","英文の場合は、単語間がスペースで区切られており、はじめから分かち書きができている状態である。また、[tensorflow.keras.preprocessing.text.Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) は、単語がスペース区切りになっている文章が前提であるため、英文では特に必要ない。\n","\n","ここでは、使用する文章のみを設定しておく。\n","```\n","# 使用する文章\n","sentences_e = [\n","    'I love my dog.',\n","    'I love my cat.',\n","    'You love my dog. I am happy.'\n","]\n","```"],"metadata":{"id":"2NVKjqI3HWvY"}},{"cell_type":"code","source":["# 使用する文章\n","sentences_e = [\n","    'I love my dog.',\n","    'I love my cat.',\n","    'You love my dog. I am happy.'\n","]"],"metadata":{"id":"WbC-mFGIyJnp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2-1-2. 単語リストの作成（英語）\n","単語に数値を割り当てるため、\n","複数の文章を入力として、\n","単語と数値（ID）のリストを作成する。\n","\n","　まず、Tokenizerのインスタンスを作成する。インスタンス作成時に、保存する単語数（```num_words```）や、辞書にない単語の取り扱い（```oov_token```）などを指定できる。\n","　次に、Tokenizerの```fit_on_text```に文章を入力し、各単語にインデックス（ID）を割り当てる。\n","\n","```\n","# ライブラリのインポート\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","# Tokenizerの作成（保存する単語数10、oovを<OOV>に設定）\n","tokenizer_e = Tokenizer(num_words = 10, oov_token='<OOV>')\n","\n","# 各単語にインデックスを割り当てる\n","tokenizer_e.fit_on_texts(sentences_e)\n","\n","# 単語リストを表示してみる\n","word_index_e = tokenizer_e.word_index\n","print(word_index_e)\n","```\n"],"metadata":{"id":"TNUC3Z4esPKz"}},{"cell_type":"code","source":["# ライブラリのインポート\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# Tokenizerの作成（保存する単語数10、oovを<OOV>に設定）\n","tokenizer_e = Tokenizer(num_words = 10, oov_token='<OOV>')\n","\n","# 各単語にインデックスを割り当てる\n","tokenizer_e.fit_on_texts(sentences_e)\n","\n","# 単語リストを表示してみる\n","word_index_e = tokenizer_e.word_index\n","word_index_e"],"metadata":{"id":"QPaSOfFftBsQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736386128089,"user_tz":-540,"elapsed":406,"user":{"displayName":"ブンカシュン文家俊","userId":"02523924470019961246"}},"outputId":"94037a02-8265-44ef-9381-236d915375d2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'<OOV>': 1,\n"," '最近だと、生成aiであいさつの例文がたくさん出てくるので、無難なものは誰でも作成できる。': 2,\n"," '求人案内の上から順に履歴書を送り、最初に内定した会社に入社しました。': 3,\n"," 'あいさつがしっかりできる人は、会社でも信頼されやすいです。': 4}"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["### 2-1-3. 文を数値リストに変換（英語）\n","単語と数値の対応表ができたので、文を数値のリストに変換することができる。また、入力文の長さを揃えるため、maxlenを超える単語数の場合はカットし、足りない場合はパディングを実施する。\n","\n","```\n","# 数値のリストに変換（上と同じsentencesを入れる場合）\n","sequences_e = tokenizer_e.texts_to_sequences(sentences_e)\n","\n","# パラメータを指定してpadding（最後をカットまたは最後にpadding。長さを10にそろえる。）\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","sequences_padded_e = pad_sequences(sequences_e, maxlen=10, padding='post', truncating='post')\n","\n","# 表示して確認\n","print(\"変換前：\", sentences_e)\n","print(\"変換表：\", word_index_e)\n","print(\"変換後：\" , sequences_e)\n","print(\"padding後：\", sequences_padded_e)\n","```"],"metadata":{"id":"-MmwIOorsuED"}},{"cell_type":"code","source":["# 数値のリストに変換（上と同じsentencesを入れる場合）\n","sequences_e = tokenizer_e.texts_to_sequences(sentences_e)\n","\n","# パラメータを指定してpadding（最後をカットまたは最後にpadding。長さを10にそろえる。）\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","sequences_padded_e = pad_sequences(sequences_e, maxlen=10, padding='post', truncating='post')\n","\n","# 表示して確認\n","print(\"変換前：\", sentences_e)\n","print(\"変換表：\", word_index_e)\n","print(\"変換後：\" , sequences_e)\n","print(\"padding後：\", sequences_padded_e)"],"metadata":{"id":"jpn90yvUsq9c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736385057273,"user_tz":-540,"elapsed":689,"user":{"displayName":"ブンカシュン文家俊","userId":"02523924470019961246"}},"outputId":"20962322-89fb-4f33-b117-1b3f302d6446"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["変換前： ['I love my dog.', 'I love my cat.', 'You love my dog. I am happy.']\n","変換表： {'<OOV>': 1, 'i': 2, 'love': 3, 'my': 4, 'dog': 5, 'cat': 6, 'you': 7, 'am': 8, 'happy': 9}\n","変換後： [[2, 3, 4, 5], [2, 3, 4, 6], [7, 3, 4, 5, 2, 8, 9]]\n","padding後： [[2 3 4 5 0 0 0 0 0 0]\n"," [2 3 4 6 0 0 0 0 0 0]\n"," [7 3 4 5 2 8 9 0 0 0]]\n"]}]},{"cell_type":"markdown","source":["## 2-2. 和文の数値化（課題）\n","日本語の場合も、英語の場合とほぼ同じ手順で数値リストに変換することができる。ただし、単語間がスペースで区切られていないため、分かち書きは別途実施する必要がある。"],"metadata":{"id":"qCWNl8t_yx6c"}},{"cell_type":"markdown","source":["### 2-2-1. 分かち書き（日本語）\n","MeCabまたはJanomeを使用して分かち書きをする。\n","\n","```\n","# 元の文のリスト\n","sentence = [\n","    '最近だと、生成AIであいさつの例文がたくさん出てくるので、無難なものは誰でも作成できる。',\n","    '求人案内の上から順に履歴書を送り、最初に内定した会社に入社しました。',\n","    'あいさつがしっかりできる人は、会社でも信頼されやすいです。'\n","]\n","\n","# MeCabまたはJanomeを使用して、分かち書き後の文（sentence_j）を作成する。\n","\n","```"],"metadata":{"id":"RTzQKqkjzLte"}},{"cell_type":"code","source":["# 元の文のリスト\n","sentence = [\n","    '最近だと、生成AIであいさつの例文がたくさん出てくるので、無難なものは誰でも作成できる。',\n","    '求人案内の上から順に履歴書を送り、最初に内定した会社に入社しました。',\n","    'あいさつがしっかりできる人は、会社でも信頼されやすいです。'\n","]"],"metadata":{"id":"acwHIjBydo2t","executionInfo":{"status":"ok","timestamp":1736388360594,"user_tz":-540,"elapsed":798,"user":{"displayName":"ブンカシュン文家俊","userId":"02523924470019961246"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["# MeCabまたはJanomeを使用して、分かち書き後の文（sentence_j）を作成する。\n","from janome.tokenizer import Tokenizer\n","\n","tokenizer = Tokenizer()\n","sentence_j = []\n","for text in sentence:\n","    tokens = [token.surface for token in tokenizer.tokenize(text)]\n","    sentence_j.append(\" \".join(tokens))\n","\n","print(sentence_j)"],"metadata":{"id":"JV9EqdIJeHVg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736388362220,"user_tz":-540,"elapsed":485,"user":{"displayName":"ブンカシュン文家俊","userId":"02523924470019961246"}},"outputId":"8fdb0122-3229-4634-f983-8dfa9c104d7f"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["['最近 だ と 、 生成 AI で あいさつ の 例文 が たくさん 出 て くる ので 、 無難 な もの は 誰 でも 作成 できる 。', '求人 案内 の 上 から 順に 履歴 書 を 送り 、 最初 に 内定 し た 会社 に 入社 し まし た 。', 'あいさつ が しっかり できる 人 は 、 会社 で も 信頼 さ れ やすい です 。']\n"]}]},{"cell_type":"markdown","source":["### 2-2-2. 単語リストの作成（日本語）\n","分かち書きができれば、以降は英文と手順は同じである。\n","Tokenizerを作成し、fit_on_textsを使用して各単語にインデックスを割り当てる。\n","**※保存する単語数は20ぐらいがいいかもしれない**"],"metadata":{"id":"jvUWVXlizP6w"}},{"cell_type":"code","source":["# ライブラリのインポート\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# Tokenizerの作成（保存する単語数10、oovを<OOV>に設定）\n","tokenizer_e = Tokenizer(num_words = 20, oov_token='<OOV>')\n","\n","# 各単語にインデックスを割り当てる\n","tokenizer_e.fit_on_texts(sentence_j)\n","\n","# 単語リストを表示してみる\n","word_index_e = tokenizer_e.word_index\n","word_index_e"],"metadata":{"id":"BnsyLH4Qv27A","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1736388453054,"user_tz":-540,"elapsed":686,"user":{"displayName":"ブンカシュン文家俊","userId":"02523924470019961246"}},"outputId":"b8e107d5-20c6-43f6-fd97-bccf76f66707"},"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'<OOV>': 1,\n"," '、': 2,\n"," '。': 3,\n"," 'で': 4,\n"," 'あいさつ': 5,\n"," 'の': 6,\n"," 'が': 7,\n"," 'は': 8,\n"," 'できる': 9,\n"," 'に': 10,\n"," 'し': 11,\n"," 'た': 12,\n"," '会社': 13,\n"," '最近': 14,\n"," 'だ': 15,\n"," 'と': 16,\n"," '生成': 17,\n"," 'ai': 18,\n"," '例文': 19,\n"," 'たくさん': 20,\n"," '出': 21,\n"," 'て': 22,\n"," 'くる': 23,\n"," 'ので': 24,\n"," '無難': 25,\n"," 'な': 26,\n"," 'もの': 27,\n"," '誰': 28,\n"," 'でも': 29,\n"," '作成': 30,\n"," '求人': 31,\n"," '案内': 32,\n"," '上': 33,\n"," 'から': 34,\n"," '順に': 35,\n"," '履歴': 36,\n"," '書': 37,\n"," 'を': 38,\n"," '送り': 39,\n"," '最初': 40,\n"," '内定': 41,\n"," '入社': 42,\n"," 'まし': 43,\n"," 'しっかり': 44,\n"," '人': 45,\n"," 'も': 46,\n"," '信頼': 47,\n"," 'さ': 48,\n"," 'れ': 49,\n"," 'やすい': 50,\n"," 'です': 51}"]},"metadata":{},"execution_count":58}]},{"cell_type":"markdown","source":["### 2-2-3. 文を数値リストに変換（日本語）\n","英文と同じように実施し、以下のような結果（maxlen=30の場合）を出力する。\n","\n","```\n","padding後： [[14 15 16  2 17 18  4  5  6 19  7 20 21 22 23 24  2 25 26 27  8 28 29 30  9  3  0  0  0  0]\n"," [31 32  6 33 34 35 36 37 38 39  2 40 10 41 11 12 13 10 42 11 43 12  3  0  0  0  0  0  0  0]\n"," [ 5  7 44  9 45  8  2 13  4 46 47 48 49 50 51  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n","   ```"],"metadata":{"id":"36M7qrVlzgLE"}},{"cell_type":"code","source":["# 数値のリストに変換（上と同じsentencesを入れる場合）\n","sequences_e = tokenizer_e.texts_to_sequences(sentence_j)\n","\n","# padding\n","sequences_padded_e = pad_sequences(sequences_e, maxlen=30, padding='post', truncating='post')\n","\n","# 結果顯示\n","print(\"padding後：\", sequences_padded_e)"],"metadata":{"id":"WL39SHEUzI2c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736388456145,"user_tz":-540,"elapsed":576,"user":{"displayName":"ブンカシュン文家俊","userId":"02523924470019961246"}},"outputId":"f800ca68-573f-4229-b135-4166844f5304"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["padding後： [[14 15 16  2 17 18  4  5  6 19  7  1  1  1  1  1  2  1  1  1  8  1  1  1\n","   9  3  0  0  0  0]\n"," [ 1  1  6  1  1  1  1  1  1  1  2  1 10  1 11 12 13 10  1 11  1 12  3  0\n","   0  0  0  0  0  0]\n"," [ 5  7  1  9  1  8  2 13  4  1  1  1  1  1  1  3  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0]]\n"]}]},{"cell_type":"markdown","source":["**pad_sequences()のパラメータを変更したり、Tokenizerのnum_wordsを小さくしたりして動作を確認し、以下を調べてみましょう。**\n","\n","**[課題]**\n","*   max_lengthより短い文の場合、padding（長さを同じにするための穴埋め）に使用される数値は何ですか。**[　0　]**\n","*   変換表にない単語を変換した場合、出力の数値は何になりますか。**[　1　]**\n","\n"],"metadata":{"id":"SyJyEv6XWJZO"}},{"cell_type":"markdown","source":["# 提出について\n","\n","以下を確認してもらうこと。\n","\n","*   和文を数値リストに変換した結果\n","*   テキストブロックへの追記（padding、辞書にない単語に使用される数値）\n","\n","ファイルが保存されているかを確認し、「ファイル＞ダウンロード＞.ipynbをダウンロード」を順にクリックして.ipynbファイルをダウンロードする。\n","\n","ダウンロードしたipynbファイルを指定の場所に提出してください。"],"metadata":{"id":"5XTKNRwqs7s5"}}]}